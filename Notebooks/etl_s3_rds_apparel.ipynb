{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Assignment\n",
    "\n",
    "__Summary__\n",
    "\n",
    "Many of Amazon's shoppers depend on product reviews to make a purchase. \n",
    "Amazon makes these datasets publicly available. However, they are quite\n",
    "large and can exceed the capacity of local machines to handle. One dataset \n",
    "alone contains over 1.5 million rows.\n",
    "\n",
    "__Goal__\n",
    "\n",
    "Performing the `ETL process` completely `in the cloud` and upload a DataFrame\n",
    "to an `RDS instance`, from two Amazon customer review data sets.\n",
    "\n",
    "__Data sets__\n",
    "\n",
    "- Shoes s3a://amazon-reviews-pds/tsv/amazon_reviews_us_Shoes_v1_00.tsv.g\n",
    "\n",
    "- Apparel s3a://amazon-reviews-pds/tsv/amazon_reviews_us_Apparel_v1_00.tsv.gz\n",
    "\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- Here is the code used in the Apache Zeppelin notebook to load the __Apparel__\n",
    "data set. Apache Zeppelin is a web-based notebook which brings data exploration, \n",
    "visualization, sharing and collaboration features to Spark.\n",
    "\n",
    "- The first data set loaded to the database was Shoes, hence here are more validations\n",
    "to ensure that the constraints were enforced, like the uniqueness in the table \n",
    "`products` or in the table `customers`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark import SparkFiles\n",
    "# Load apparel data from S3 into a DataFrame\n",
    "\n",
    "apparel_df = spark.read.option('header', 'true').csv(\"s3a://amazon-reviews-pds/tsv/amazon_reviews_us_Apparel_v1_00.tsv.gz\", inferSchema=True, sep='\\t')\n",
    "apparel_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# review_date should be in the format yyyy-mm-dd\n",
    "date_df = apparel_df.withColumn(\"date\", to_date(\"review_date\", \"yyyy-mm-dd\"))\n",
    "date_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import col\n",
    "# Created data frame to match review_id_table\n",
    "# CREATE TABLE review_id_table (review_id TEXT PRIMARY KEY NOT NULL, customer_id INTEGER, product_id TEXT, product_parent INTEGER, \n",
    "# review_date DATE -- this should be in the formate yyyy-mm-dd );\n",
    "\n",
    "review_df = date_df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", col(\"date\").alias(\"review_date\")])\n",
    "review_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Number of reviews\n",
    "review_df.count()  # => 5906333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Created data frame to match products table  -- This table will contain only unique values\n",
    "# CREATE TABLE products (product_id TEXT PRIMARY KEY NOT NULL UNIQUE,product_title TEXT);\n",
    "\n",
    "products_df = date_df.select([\"product_id\", \"product_title\"]).distinct()\n",
    "products_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Number of products\n",
    "products_df.count()   # => 2305630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Created data frame to match customer table -- Customer table for first data set\n",
    "# CREATE TABLE customers (customer_id INT PRIMARY KEY NOT NULL UNIQUE,  customer_count INT);\n",
    "\n",
    "counts_df = date_df.groupBy(\"customer_id\").count().orderBy(\"customer_id\")\n",
    "counts_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Check the data types\n",
    "counts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Change the name of the column and the data type\n",
    "customers_df =  counts_df.select([\"customer_id\", col(\"count\").cast(IntegerType()).alias(\"customer_count\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Check the changes\n",
    "customers_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "customers_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Number of customers\n",
    "customers_df.count()  # ->3228415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Created data frame to match vine table\n",
    "# CREATE TABLE vine_table (review_id TEXT PRIMARY KEY, star_rating INTEGER, helpful_votes INTEGER, total_votes INTEGER, vine TEXT);\n",
    "\n",
    "vine_df = date_df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"])\n",
    "vine_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Number of vines\n",
    "vine_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import col\n",
    "# Divide data set of vines to ensure the database insert\n",
    "\n",
    "vine1_df = vine_df.filter(col(\"star_rating\") == 1)\n",
    "vine1_df.count()  # => 445456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Divide data set of vines to ensure the database insert\n",
    "vine2_df = vine_df.filter(col(\"star_rating\") == 2)\n",
    "vine2_df.count()  # => 369601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Divide data set of vines to ensure the database insert\n",
    "vine3_df = vine_df.filter(col(\"star_rating\") == 3)\n",
    "vine3_df.count()   # => 623471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import col\n",
    "# Divide data set of vines to ensure the database insert\n",
    "vine4_df = vine_df.filter(col(\"star_rating\") == 4)\n",
    "vine4_df.count()   # => 1147237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Divide data set of vines to ensure the database insert\n",
    "vine5_df = vine_df.filter(col(\"star_rating\") == 5)\n",
    "vine5_df.count()  # => 3320557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://<endpoiny>:<port>/<db>\"\n",
    "config = {\"user\":\"<user>\", \n",
    "          \"password\": \"<pwd>\", \n",
    "          \"driver\":\"org.postgresql.Driver\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Append DataFrame to review_id_table in RDS\n",
    "review_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Records from shoes and apparel data sets__\n",
    "\n",
    "![Review](../Images/rev_sho_app.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Get the products from the database\n",
    "dbproducts_df = sqlContext.read.jdbc(url=jdbc_url, table='products', properties=config)\n",
    "dbproducts_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Join the two products data frames (right excluding join to have only the products that are not in the database)\n",
    "new_prod_df = products_df.join(dbproducts_df, on=\"product_id\", how=\"leftanti\")\n",
    "new_prod_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# See the product that raises an error during insert\n",
    "new_prod_df.filter(new_prod_df.product_id == 'B00IVCPVNK').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Drop duplicates of the new data frame that came from apparel data set\n",
    "new_prod_df = new_prod_df.select(['product_id', 'product_title']).dropDuplicates(['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Check again the product that raises an error during insert\n",
    "new_prod_df.filter(new_prod_df.product_id == 'B00IVCPVNK').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Number of new unique products\n",
    "new_prod_df.count()   #1798374  -- 1798371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Write dataframe to products table in RDS\n",
    "new_prod_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Records from shoes and apparel data sets__\n",
    "\n",
    "![Product](../Images/prod_sho_app.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Get the customers from the database\n",
    "dbcustomers_df = sqlContext.read.jdbc(url=jdbc_url, table=\"customers\", properties=config)\n",
    "dbcustomers_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "dbcustomers_df.count()  # => 2816830"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Join the two customers data frames (right excluding join to have only the customers that are not in the database)\n",
    "new_cust_df = customers_df.join(dbcustomers_df, on=\"customer_id\" , how=\"leftanti\")\n",
    "new_cust_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Check if are duplicates by customer_id before trying to insert the records\n",
    "if new_cust_df.count() > new_cust_df.dropDuplicates(['customer_id']).count():\n",
    "    raise ValueError('Data has duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "new_cust_df.count()  # => 2366059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Select customers that already were in the database\n",
    "same_cust_df = customers_df.select('customer_id', 'customer_count')\n",
    "same_cust_df = same_cust_df.withColumnRenamed('customer_count', 'cdb_count')\n",
    "same_cust_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Join the two customers data frames (see which customers are in the database and in the data set)\n",
    "same_c_df = same_cust_df.join(dbcustomers_df,  on=\"customer_id\" , how=\"inner\")\n",
    "same_c_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "same_c_df.count()   # => 862356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Write dataframe to customers table in RDS\n",
    "new_cust_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Records from shoes and apparel data sets__\n",
    "\n",
    "![Customer](../Images/cust_sho_app.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Write dataframe to customers table in RDS\n",
    "same_c_df.write.jdbc(url=jdbc_url, table='repcustomers', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Write dataframe to vine_table_app table in RDS\n",
    "vine1_df.write.jdbc(url=jdbc_url, table='vine_table_app', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Write dataframe to vine_table_app table in RDS\n",
    "vine2_df.write.jdbc(url=jdbc_url, table='vine_table_app', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Write dataframe to vine_table_app table in RDS\n",
    "vine3_df.write.jdbc(url=jdbc_url, table='vine_table_app', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Write dataframe to vine_table_app table in RDS\n",
    "vine4_df.write.jdbc(url=jdbc_url, table='vine_table_app', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# Write dataframe to vine_table_app table in RDS\n",
    "vine5_df.write.jdbc(url=jdbc_url, table='vine_table_app', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Only records from apparel data set__\n",
    "\n",
    "![Vine](../Images/vine_app.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
